import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

def main():
    # Set the cache directory for all downloads
    cache_dir = "/Data/my_project_env2/huggingface_cache"
    os.environ["TRANSFORMERS_CACHE"] = cache_dir
    os.environ["HF_HOME"] = cache_dir
    os.environ["TORCH_HOME"] = cache_dir

    device = "cuda" if torch.cuda.is_available() else "cpu"
    # Use the smaller IBM Granite model for code generation
    model_path = "ibm-granite/granite-3b-code-instruct-2k"

    # Read the description from the file generated by blip.py
    with open("clip_description.txt", "r") as f:
        description = f.read().strip()

    # Load the IBM Granite model and tokenizer with the custom cache directory
    tokenizer = AutoTokenizer.from_pretrained(model_path, cache_dir=cache_dir)
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        cache_dir=cache_dir,
        device_map="auto" if device == "cuda" else None
    )
    model.eval()
    # Do not call model.to(device) because the model is already dispatched via Accelerate

    # Construct a prompt to generate Python code using Open3D based on the description
    prompt = (
        f"Based on the following description of an object, generate Python code that uses Open3D "
        f"to create a 3D model of the object.\n\n"
        f"Description: {description}\n\n"
        "Python code:"
    )

    # Format the prompt using the model's chat template
    chat = [{"role": "user", "content": prompt}]
    chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)

    # Tokenize the prompt and move inputs to the device
    input_tokens = tokenizer(chat, return_tensors="pt").to(device)

    try:
        # Generate output tokens (adjust max_new_tokens as needed)
        output_tokens = model.generate(**input_tokens, max_new_tokens=200)
    except Exception as e:
        error_message = str(e)
        print("An error occurred during generation:")
        print(error_message)
        # Provide some basic error correction suggestions based on the error content
        if "CUDA out of memory" in error_message:
            print("Suggestion: Reduce 'max_new_tokens', lower the batch size, or set the environment variable PYTORCH_CUDA_ALLOC_CONF to adjust memory allocation.")
        else:
            print("Suggestion: Check that your input is formatted correctly and that the model supports your request.")
        return

    generated_code = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)[0]

    print("Generated Code:")
    print(generated_code)

if __name__ == "__main__":
    main()
